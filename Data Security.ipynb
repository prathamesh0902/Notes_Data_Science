{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing\n",
    "\n",
    "**Definition**: Hashing is the process of converting an input (or 'message') into a fixed-size string of bytes, typically a digest that appears random. This is done using a hash function.\n",
    "\n",
    "**Purpose**: Hashing is commonly used in data structures such as hash tables, for checking data integrity, and for securely storing passwords.\n",
    "\n",
    "**Properties**:\n",
    "- **Deterministic**: The same input will always produce the same output.\n",
    "- **Fixed Size**: The output (hash) is always of a fixed length, regardless of the size of the input.\n",
    "- **Efficient**: The hash function should be able to return the hash value quickly.\n",
    "- **Pre-image Resistance**: It should be computationally infeasible to reverse the hash function.\n",
    "- **Collision Resistance**: It should be difficult to find two different inputs that produce the same hash output.\n",
    "\n",
    "**Example**: SHA-256, MD5\n",
    "\n",
    "### Salting\n",
    "\n",
    "**Definition**: Salting is the process of adding random data (a 'salt') to the input of a hash function.\n",
    "\n",
    "**Purpose**: Salting is primarily used to protect against dictionary attacks and rainbow table attacks on hashed passwords. By adding a unique salt to each password, the hash output is different even if the same password is used.\n",
    "\n",
    "**Properties**:\n",
    "- **Uniqueness**: Each salt should be unique for each password.\n",
    "- **Randomness**: The salt should be randomly generated to ensure security.\n",
    "- **Storage**: The salt needs to be stored alongside the hashed password so that it can be used for verification later.\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Password: password123\n",
    "Salt: abc123\n",
    "Hashed Output: Hash(password123abc123)\n",
    "```\n",
    "\n",
    "### Encryption and Decryption\n",
    "\n",
    "**Definition**: Encryption is the process of converting plaintext into ciphertext using an algorithm and an encryption key. Decryption is the reverse process, converting the ciphertext back to plaintext using a decryption key.\n",
    "\n",
    "**Purpose**: The main purpose of encryption and decryption is to ensure the confidentiality of data, making it unreadable to unauthorized users.\n",
    "\n",
    "**Types**:\n",
    "- **Symmetric Encryption**: The same key is used for both encryption and decryption. Example algorithms: AES, DES.\n",
    "- **Asymmetric Encryption**: Uses a pair of keys, a public key for encryption and a private key for decryption. Example algorithms: RSA, ECC.\n",
    "\n",
    "**Properties**:\n",
    "- **Confidentiality**: Ensures that the information is only accessible to those authorized to access it.\n",
    "- **Integrity**: Ensures that the information has not been altered during transmission.\n",
    "- **Authentication**: Verifies the identity of the parties involved in the communication.\n",
    "- **Non-repudiation**: Ensures that a sender cannot deny sending a message.\n",
    "\n",
    "**Example**:\n",
    "- **Symmetric Encryption**:\n",
    "  ```\n",
    "  Plaintext: HelloWorld\n",
    "  Key: secretkey\n",
    "  Ciphertext: EncryptedText\n",
    "  ```\n",
    "- **Asymmetric Encryption**:\n",
    "  ```\n",
    "  Plaintext: HelloWorld\n",
    "  Public Key: publickey\n",
    "  Ciphertext: EncryptedText\n",
    "  Private Key: privatekey\n",
    "  ```\n",
    "\n",
    "**Decryption**:\n",
    "```\n",
    "Ciphertext: EncryptedText\n",
    "Key: secretkey (symmetric) or privatekey (asymmetric)\n",
    "Plaintext: HelloWorld\n",
    "```\n",
    "\n",
    "In summary, hashing and salting are techniques mainly used for data integrity and securely storing passwords, while encryption and decryption are techniques used to ensure data confidentiality and secure communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For managing various stages like **requirements gathering**, **project scoping**, **SLA management**, **implementation**, **delivery**, and **feedback implementation**, a combination of tools is typically used across different stages of the project lifecycle. Hereâ€™s an overview of the tools typically required:\n",
    "\n",
    "### 1. **Requirements Gathering**:\n",
    "   - **JIRA**: Commonly used for capturing and tracking requirements in Agile environments. It allows collaboration between stakeholders and the development team.\n",
    "   - **Confluence**: A documentation tool often used in conjunction with JIRA to gather and document detailed project requirements and scope.\n",
    "   - **Trello/Asana**: Light-weight project management tools for brainstorming, capturing ideas, and organizing requirements visually.\n",
    "   - **Microsoft Teams/Slack**: For collaboration and gathering informal feedback from stakeholders.\n",
    "   - **Microsoft Word/Excel**: For creating and sharing formal requirement documents.\n",
    "   - **Miro/Lucidchart**: For visual mapping of requirements (use case diagrams, workflows, etc.).\n",
    "\n",
    "### 2. **Project Scoping**:\n",
    "   - **Microsoft Project**: Used for project planning and scoping, with Gantt charts and resource allocation.\n",
    "   - **Smartsheet**: A collaborative tool that helps manage and visualize the scope, timelines, and deliverables.\n",
    "   - **Aha!**: A product management platform for capturing strategy and roadmaps, especially for large projects.\n",
    "   - **Google Docs**: For drafting project scopes and allowing multiple users to collaborate on the scope document.\n",
    "   - **Wrike**: Useful for breaking down the project scope into tasks, timelines, and responsible individuals.\n",
    "\n",
    "### 3. **SLA Management**:\n",
    "   - **ServiceNow**: A comprehensive platform for managing SLAs, particularly in service-oriented projects.\n",
    "   - **Zendesk/Freshdesk**: Customer support platforms that allow SLA tracking and performance analysis.\n",
    "   - **Salesforce Service Cloud**: Helps with SLA monitoring in customer relationship management.\n",
    "   - **BMC Remedy**: Used for service management and tracking SLA compliance.\n",
    "   - **Excel/Google Sheets**: For custom SLA tracking and reporting in smaller setups.\n",
    "\n",
    "### 4. **Implementation**:\n",
    "   - **JIRA**: For managing sprints, assigning tasks, and tracking progress during the implementation phase.\n",
    "   - **Git/GitHub/GitLab**: Version control systems for managing code during development and implementation.\n",
    "   - **Docker/Kubernetes**: For containerizing applications and managing their deployment.\n",
    "   - **Jenkins/CI-CD pipelines**: Continuous Integration/Continuous Delivery tools for automating deployment.\n",
    "   - **AWS/Azure/GCP**: Cloud platforms where the implementation might be deployed for infrastructure setup.\n",
    "   - **Ansible/Chef/Puppet**: Automation tools used in DevOps to facilitate the implementation.\n",
    "\n",
    "### 5. **Delivery**:\n",
    "   - **Jenkins**: For automating deployment and delivery pipelines.\n",
    "   - **Docker**: For ensuring consistent application delivery across different environments.\n",
    "   - **Azure DevOps**: For project management and end-to-end delivery, including automated testing, build, and deployment.\n",
    "   - **Bitbucket/GitLab**: For version control and facilitating delivery processes.\n",
    "   - **JIRA**: For closing out tasks related to delivery and final handoff.\n",
    "   - **Slack/Microsoft Teams**: For coordinating the delivery, setting up communication channels for go-live updates.\n",
    "\n",
    "### 6. **Feedback Implementation**:\n",
    "   - **SurveyMonkey/Google Forms**: For collecting structured feedback from users or stakeholders.\n",
    "   - **JIRA Service Management**: For managing feedback tickets, resolving issues, and implementing change requests.\n",
    "   - **Confluence**: For documenting and discussing feedback implementation with team members.\n",
    "   - **Miro**: For visual feedback and collaboration on project retrospectives.\n",
    "   - **Power BI/Tableau**: For analyzing feedback trends and generating reports on customer satisfaction or system performance.\n",
    "   - **Hotjar/UserTesting**: For gathering user feedback on websites or applications to identify areas of improvement.\n",
    "\n",
    "### 7. **Automating data science models and job scheduling**:\n",
    "\n",
    "1. **Airflow**: A platform to programmatically author, schedule, and monitor workflows.\n",
    "2. **Luigi**: A Python module that helps with workflow management and job scheduling.\n",
    "3. **Kubeflow**: A machine learning toolkit for Kubernetes that helps with model deployment, pipeline automation, and orchestration.\n",
    "4. **Prefect**: A modern workflow orchestration tool that integrates with Python and handles scheduling and error handling.\n",
    "5. **Dask**: A parallel computing library that integrates well with Python and is useful for large-scale data processing.\n",
    "6. **Jenkins**: A CI/CD tool that can automate ML pipeline deployments.\n",
    "7. **MLflow**: A tool that helps with automating machine learning model lifecycle (training, tracking, deployment).\n",
    "8. **Cron Jobs** (Linux): For scheduling scripts or processes at regular intervals.\n",
    "9. **Apache Spark**: Great for distributed data processing and can be integrated with job schedulers like Airflow.\n",
    "\n",
    "In practice, the selection of tools depends on the specific requirements, scale of the project, and organizational preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For managing various stages like **requirements gathering**, **project scoping**, **SLA management**, **implementation**, **delivery**, and **feedback implementation**, a combination of tools is typically used across different stages of the project lifecycle. Hereâ€™s an overview of the tools typically required:\n",
    "\n",
    "### 1. **Requirements Gathering**:\n",
    "   - **Microsoft Teams/Slack**: For collaboration and gathering informal feedback from stakeholders.\n",
    "   - **Microsoft Word/Excel**: For creating and sharing formal requirement documents.\n",
    "\n",
    "### 2. **Project Scoping**:\n",
    "   - **Google Docs**: For drafting project scopes and allowing multiple users to collaborate on the scope document.\n",
    "\n",
    "\n",
    "### 3. **SLA Management**:\n",
    "   - **ServiceNow**: A comprehensive platform for managing SLAs, particularly in service-oriented projects.\n",
    "   - **Excel/Google Sheets**: For custom SLA tracking and reporting in smaller setups.\n",
    "\n",
    "### 4. **Implementation**:\n",
    "   - **JIRA**: For managing sprints, assigning tasks, and tracking progress during the implementation phase.\n",
    "   - **Git/GitHub/GitLab**: Version control systems for managing code during development and implementation.\n",
    "\n",
    "### 5. **Delivery**:\n",
    "   - **Docker**: For ensuring consistent application delivery across different environments.\n",
    "   - **Azure DevOps**: For project management and end-to-end delivery, including automated testing, build, and deployment.\n",
    "   - **JIRA**: For closing out tasks related to delivery and final handoff.\n",
    "\n",
    "\n",
    "### 6. **Feedback Implementation**:\n",
    "   - **SurveyMonkey/Google Forms**: For collecting structured feedback from users or stakeholders.\n",
    "\n",
    "### 7. **Automating data science models and job scheduling**:\n",
    "9. **Apache Spark**: Great for distributed data processing and can be integrated with job schedulers like Airflow.\n",
    "\n",
    "In practice, the selection of tools depends on the specific requirements, scale of the project, and organizational preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a comparison table between your **Olympic Data Analysis Project on Azure Synapse, Databricks, and PySpark** and a similar project using **GCP and Hadoop**:\n",
    "\n",
    "| **Aspect**                        | **Azure Synapse, Databricks, and PySpark** | **GCP with Hadoop (Dataproc)**                       | **Similarities**                                   |\n",
    "|-----------------------------------|--------------------------------------------|------------------------------------------------------|----------------------------------------------------|\n",
    "| **Data Storage**                  | Azure Data Lake / Azure Blob Storage       | Google Cloud Storage (GCS)                           | Both platforms provide scalable, cloud-based storage solutions for large datasets. |\n",
    "| **Data Processing Framework**     | PySpark (on Databricks)                    | Hadoop with MapReduce / Spark (on Dataproc)          | Both environments support distributed data processing and offer Spark for faster processing. |\n",
    "| **Cluster Management**            | Azure Synapse Analytics / Databricks       | Google Cloud Dataproc                                | Both offer managed services to handle distributed clusters for processing big data. |\n",
    "| **Job Scheduling**                | Databricks Jobs / Azure Data Factory       | Cloud Composer (Airflow) / Cloud Scheduler           | Both provide job orchestration and scheduling services to automate ETL workflows. |\n",
    "| **Query Engine**                  | Synapse SQL Pool, Spark SQL                | BigQuery (for querying processed data)               | Both systems allow SQL-like queries over large datasets post-processing. |\n",
    "| **Data Visualization**            | Power BI, Azure Synapse Analytics Studio   | Google Data Studio / Looker                          | Both have integrated visualization tools for creating dashboards and reports. |\n",
    "| **Scalability**                   | Auto-scaling via Databricks Clusters       | Auto-scaling via Dataproc                            | Both systems offer dynamic scaling of resources based on processing needs. |\n",
    "| **Data Processing Language**      | PySpark, SQL                              | Hadoop (MapReduce), PySpark                          | PySpark is available in both platforms, allowing for consistent data processing syntax. |\n",
    "| **Automation**                    | Databricks Notebooks, Azure Logic Apps     | Cloud Functions, Cloud Composer (Airflow)            | Both allow automating workflows and triggering tasks based on events. |\n",
    "| **Managed Services**              | Databricks (fully managed)                 | Google Cloud Dataproc (fully managed)                | Both are fully managed services for distributed data processing, abstracting infrastructure management. |\n",
    "\n",
    "### Key Similarities:\n",
    "- **Data Storage**: Both Azure and GCP offer highly scalable, secure cloud-based storage solutions for managing large volumes of data.\n",
    "- **Distributed Data Processing**: Both environments use Apache Spark for distributed data processing, providing familiarity in how data is handled, processed, and transformed.\n",
    "- **Cluster Management**: Databricks and Dataproc are both managed services for running distributed computing jobs, offering automatic scaling and cluster lifecycle management.\n",
    "- **Automation & Orchestration**: Tools like Airflow (Cloud Composer) and Databricks Jobs/Data Factory help in orchestrating ETL pipelines, offering similar job scheduling functionality.\n",
    "- **SQL Query Engines**: Both Azure (with Synapse SQL Pool) and GCP (with BigQuery) provide powerful query engines for querying large datasets post-processing.\n",
    "\n",
    "While the services and tool names may differ between Azure and GCP, the underlying concepts and workflows remain quite similar. Would you like more information on transitioning between the two platforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a table comparing the similarities between **Hadoop** and **Apache Spark**:\n",
    "\n",
    "| **Feature**                | **Hadoop**                               | **Apache Spark**                          | **Similarity**                              |\n",
    "|----------------------------|------------------------------------------|-------------------------------------------|---------------------------------------------|\n",
    "| **Distributed Data Processing** | Uses HDFS (Hadoop Distributed File System) for storing and processing data across multiple nodes | Uses in-memory processing across multiple nodes | Both platforms support distributed data processing across clusters |\n",
    "| **Programming Model**       | MapReduce for processing large datasets | Resilient Distributed Datasets (RDD) and DataFrame APIs | Both rely on functional programming models for parallel data processing |\n",
    "| **Fault Tolerance**         | Provides fault tolerance via replication in HDFS | Provides fault tolerance using lineage of transformations and data re-computation | Both offer fault tolerance to recover from failures |\n",
    "| **Scalability**             | Scales out horizontally by adding nodes | Scales out horizontally by adding nodes | Both are horizontally scalable over large clusters |\n",
    "| **Supported Languages**     | Supports Java, Python, and Scala | Supports Java, Python, Scala, and R | Both support multiple programming languages |\n",
    "| **Batch Processing**        | Processes data in batch mode using MapReduce | Supports batch processing via Spark Core | Both support large-scale batch processing |\n",
    "| **Integration with HDFS**   | Natively integrated with HDFS for storage | Can read and write data from HDFS | Both can integrate and work with HDFS |\n",
    "| **Data Locality**           | Moves computation closer to where the data is stored (data locality) | Supports data locality by running computation where the data resides | Both leverage data locality to improve performance |\n",
    "| **Job Scheduling**          | Uses YARN (Yet Another Resource Negotiator) for resource management and job scheduling | Can also integrate with YARN or Mesos for cluster management | Both can be used with YARN for job scheduling |\n",
    "| **Ecosystem Tools**         | Part of the broader Hadoop ecosystem with tools like Hive, Pig, and HBase | Can integrate with Hadoop ecosystem tools like Hive and HBase | Both integrate well with the broader Hadoop ecosystem for data storage and querying |\n",
    "\n",
    "While Hadoop primarily uses **disk-based** processing with **MapReduce**, Spark enhances performance by allowing **in-memory** processing, but they share several core concepts around distributed and fault-tolerant data processing in large clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'yellow'> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
